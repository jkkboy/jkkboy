[ Multi-variable linear regression (1)]
# data and label
x1 = [ 73.,  93.,  89.,  96.,  73.]           # 입력데이터, 정답(=레이블, Y) 입력
x2 = [ 80.,  88.,  91.,  98.,  66.]
x3 = [ 75.,  93.,  90., 100.,  70.]
Y  = [152., 185., 180., 196., 142.]

# random weights
w1 = tf.Variable(tf.random_normal([1]))       # 변수(x)가 3개니 W도 3개 필요 , b는 1개
w2 = tf.Variable(tf.random_normal([1]))       # 초기값 [1] (초기값은 아무거나 줘도 됨)
w3 = tf.Variable(tf.random_normal([1]))
b  = tf.Variable(tf.random_normal([1]))

learning_rate = 0.000001

for i in range(1000+1):                                                           # 아래 과정을 1001번 실행
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:                                               # GradientTape 학습하고 tape에 기록
        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b
        cost = tf.reduce_mean(tf.square(hypothesis - Y))                          # cost는 (가설-실제값) 제곱의 평균값
    # calculates the gradients of the cost
    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])      # tape 불러와서 cost에 대한 w1~4의 기울기를 구함
    
    # update w1,w2,w3 and b
    w1.assign_sub(learning_rate * w1_grad)              # w1 = w1 - w1기울기*학습률
    w2.assign_sub(learning_rate * w2_grad)              # w2 = w2 - w2기울기*학습률
    w3.assign_sub(learning_rate * w3_grad)
    b.assign_sub(learning_rate * b_grad)

    if i % 50 == 0:                                     # 50번 마다 출력
      print("{:5} | {:12.4f}".format(i, cost.numpy()))
    0 |   11325.9121                                    # 처음 매우 큰 값이 급격히 줄어들다 변동이 거의 없어짐
   50 |     135.3618
  100 |      11.1817
  150 |       9.7940
  200 |       9.7687
  250 |       9.7587
  300 |       9.7489
  350 |       9.7389
  400 |       9.7292
  450 |       9.7194
  500 |       9.7096
  550 |       9.6999
  600 |       9.6903
  650 |       9.6806
  700 |       9.6709
  750 |       9.6612
  800 |       9.6517
  850 |       9.6421
  900 |       9.6325
  950 |       9.6229
 1000 |       9.6134
 
 
 
 
 [ Multi-variable linear regression (2)]
 data = np.array([                        #x, y데이터 입력
    # X1,   X2,    X3,   y
    [ 73.,  80.,  75., 152. ],
    [ 93.,  88.,  93., 185. ],
    [ 89.,  91.,  90., 180. ],
    [ 96.,  98., 100., 196. ],
    [ 73.,  66.,  70., 142. ]
], dtype=np.float32)

# slice data
X = data[:, :-1]                          # : 주변에 아무 수식 없으니 전체 (: = 행) , :-1 은 마지막 컬럼? 제외 즉, X데이터는 X1~3 까지
y = data[:, [-1]]                         # [-1] = 마지막 열만 사용 즉, Y데이터는 y행만

W = tf.Variable(tf.random_normal([3, 1]))               # x값은 x1~3까지 3개이고 출력값은 1개이니 [3,1]
b = tf.Variable(tf.random_normal([1]))  

learning_rate = 0.000001

# hypothesis, prediction function
def predict(X):                           # 예측(가설) WX + b
    return tf.matmul(X, W) + b

print("epoch | cost")

n_epochs = 2000
for i in range(n_epochs+1):                                                 # 2001회 실행
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:                                         # tape에 기록
        cost = tf.reduce_mean((tf.square(predict(X) - y)))                  # (가설(x) - 실제) 제곱의 평균이 cost

    # calculates the gradients of the loss
    W_grad, b_grad = tape.gradient(cost, [W, b])                            # tape에 기록된 함수에 대한 w,b의 기울기

    # updates parameters (W and b)
    W.assign_sub(learning_rate * W_grad)                                    # W = W - W기울기*학습률
    b.assign_sub(learning_rate * b_grad)
    
    if i % 100 == 0:                                                        # 100번마다 출력
        print("{:5} | {:10.4f}".format(i, cost.numpy()))
epoch | cost
    0 |  5455.5903                                                          # 진행될 수록 cost값이 줄어듬
  100 |    31.7443
  200 |    30.9326
  300 |    30.7894
  400 |    30.6468
  500 |    30.5055
  600 |    30.3644
  700 |    30.2242
  800 |    30.0849
  900 |    29.9463
 1000 |    29.8081
 1100 |    29.6710
 1200 |    29.5348
 1300 |    29.3989
 1400 |    29.2641
 1500 |    29.1299
 1600 |    28.9961
 1700 |    28.8634
 1800 |    28.7313
 1900 |    28.5997
 2000 |    28.4689
 
